{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIx-nje0Z3L1"
      },
      "outputs": [],
      "source": [
        "!pip install trl\n",
        "!pip intall transformers\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG9PdTeaarTX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import RewardTrainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1dA5yyEwuDg"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"CarperAI/openai_summarize_comparisons\"\n",
        "prepared_dataset = load_dataset(DATA_PATH, split=\"train[:10000]\")\n",
        "prepared_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8aYRFH2Rw2QY"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilroberta-base\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
        "    prompt_plus_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "formatted_dataset = prepared_dataset.map(formatting_func)\n",
        "formatted_dataset = formatted_dataset.train_test_split()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/rm\",\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=500\n",
        ")\n",
        "\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset[\"train\"],\n",
        "    eval_dataset=formatted_dataset[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}