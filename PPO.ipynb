{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf1NT0hK6Esm"
      },
      "outputs": [],
      "source": [
        "!pip install trl\n",
        "!pip install transformers\n",
        "!pip install argilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8FhrTSBP0gV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "from trl.core import LengthSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dSop5tQOFI"
      },
      "outputs": [],
      "source": [
        "reward_model = \"argilla/roberta-base-reward-model-falcon-dolly\"\n",
        "reward_tokenizer = \"argilla/roberta-base-reward-model-falcon-dolly\"\n",
        "\n",
        "config = PPOConfig(model_name=\"gpt2\", batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gdMzJOPQSH5"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "reward_pipe = ... # use pipeline to create reward pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11_vpGF9QT1n"
      },
      "outputs": [],
      "source": [
        "import argilla as rg\n",
        "from datasets import Dataset\n",
        "\n",
        "feedback_dataset = rg.FeedbackDataset.from_huggingface(\"argilla/databricks-dolly-15k-curated-en\")\n",
        "\n",
        "data = {\"instruction\": [], \"context\": [], \"response\": []}\n",
        "for entry in feedback_dataset:\n",
        "    if entry.responses:\n",
        "        res = entry.responses[0].values\n",
        "        data[\"instruction\"].append(res[\"new-instruction\"].value)\n",
        "        data[\"context\"].append(res[\"new-context\"].value)\n",
        "        data[\"response\"].append(res[\"new-response\"].value)\n",
        "\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ChSgnwjdQVl6"
      },
      "outputs": [],
      "source": [
        "def formatting_func(examples):\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\", \"truncation\": True,\n",
        "        \"max_length\": 512, \"return_tensors\": \"pt\"\n",
        "    }\n",
        "    input_size = LengthSampler(min_value=2, max_value=8)\n",
        "    input_text = examples[\"instruction\"] + examples[\"context\"] + examples[\"response\"]\n",
        "    examples[\"input_ids\"] = tokenizer.encode(input_text, **kwargs)[0][: input_size()]\n",
        "    examples[\"query\"] = tokenizer.decode(examples[\"input_ids\"][0])\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSZ4tHCiQWT1"
      },
      "outputs": [],
      "source": [
        "formatted_dataset = dataset.map(formatting_func, batched=False)\n",
        "formatted_dataset.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krBYgTjpQZXu"
      },
      "outputs": [],
      "source": [
        "def collator(data): # you always need a collator for policy models\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV-FuuRWQbFX"
      },
      "outputs": [],
      "source": [
        "ppo_trainer = ... # PPO configuration\n",
        "\n",
        "output_min_length = 4\n",
        "output_max_length = 16\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsb8BCHQQcpl"
      },
      "outputs": [],
      "source": [
        "for epoch, batch in enumerate(ppo_trainer.dataloader):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    #### Get response from gpt2\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        gen_len = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "        response_tensors.append(response.squeeze()[-gen_len:])\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    #### Compute sentiment score\n",
        "    texts = [...] # create the text to get the reward\n",
        "    pipe_outputs = reward_pipe(texts, return_all_scores=True)\n",
        "    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
        "\n",
        "    #### Run PPO step\n",
        "    stats = ... # query, reponse and rewards\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}